# -*- coding: utf-8 -*-
"""classification test for lending club.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JnivYdR27XFVFkG347l_RQAdgPlA_U8v

https://www.kaggle.com/zaurbegiev/my-dataset
"""

from pycaret.utils import enable_colab
enable_colab()

!pip install pycaret

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

## load dataset from google drive

from google.colab import drive
drive.mount('/content/drive')

dataset=pd.read_csv("/content/sample_data/lc_cleaned_combined.csv") # dataset location on Drive

dataset.head()

!pip install pandas-profiling

from pandas_profiling import ProfileReport
profile = ProfileReport(dataset, title='Pandas Profiling Report')

profile

profile.to_file('lc_data_profile.html')

#check the shape of data
dataset.shape

data = dataset.sample(frac=0.9, random_state=786).reset_index(drop=True)
data_unseen = dataset.drop(data.index)

data.reset_index(drop=True,inplace=True)
data_unseen.reset_index(drop=True,inplace=True)

print('Data for Modeling: ' + str(data.shape))
print('Unseen Data For Predictions: ' + str(data_unseen.shape))

data.columns

#for classification purpose
from pycaret.classification import *
#?setup

exp_clf101 = setup(data = data, target = 'loan_status', session_id=123,train_size= 0.8, ignore_features=['Unnamed: 0', 'funded_amnt', 'funded_amnt_inv', 
       'emp_title', 
       'issue_d',   'addr_state', 'dti',
       'delinq_2yrs'])
                   #numeric_features=['pub_rec'],  
##exclude unimporatant predictors

"""# 7.0 Comparing All Models

Comparing all models to evaluate performance is the recommended starting point for modeling once the setup is completed (unless you exactly know what kind of model you need, which is often not the case). This function trains all models in the model library and scores them using stratified cross validation for metric evaluation. The output prints a score grid that shows average Accuracy, AUC, Recall, Precision, F1 and Kappa accross the folds (10 by default) of all the available models in the model library.
"""

?compare_models
#ignore xgboost model for an error issue with column name

# k=5 Cross-validation for speedy output for now, ignore 'xgboost' model becuase we have "["or "]" or "," in emp_year

compare_models(exclude=["xgboost"])

"""Two simple words of code ***(not even a line)*** have created over 15 models using 10 fold stratified cross validation and evaluated the 6 most commonly used classification metrics (Accuracy, AUC, Recall, Precision, F1, Kappa). The score grid printed above highlights the highest performing metric for comparison purposes only. The grid by default is sorted using 'Accuracy' (highest to lowest) which can be changed by passing the `sort` parameter. For example `compare_models(sort = 'Recall')` will sort the grid by Recall instead of Accuracy. If you want to change the fold parameter from the default value of `10` to a different value then you can use the `fold` parameter. For example `compare_models(fold = 5)` will compare all models on 5 fold cross validation. Reducing the number of folds will improve the training time.

# 8.0 Create a Model
"""

rf=create_model('rf')

lr=create_model('lr')

"""### 8.1 confirm Classifier based on last session"""

#apparently logistic regression is good enough
lr=create_model('lr')

#random forest
rf=create_model('rf')
#SVM

?create_model

#trained model object is stored in the variable 'dt'. 
print(rf)
print(lr)

"""# 9.0 Tune a Model

When a model is created using the `create_model()` function it uses the default hyperparameters. In order to tune hyperparameters, the `tune_model()` function is used. This function automatically tunes the hyperparameters of a model on a pre-defined search space and scores it using stratified cross validation. The output prints a score grid that shows Accuracy, AUC, Recall, Precision, F1 and Kappa by fold. <br/>
<br/>
**Note:** `tune_model()` does not take a trained model object as an input. It instead requires a model name to be passed as an abbreviated string similar to how it is passed in `create_model()`. All other functions in `pycaret.classification` require a trained model object as an argument.

### 9.1 Tune model
"""

tuned_lr=tune_model(lr,optimize="Accuracy")

tuned_rf = tune_model(rf,optimize='Accuracy')

#tuned model object is stored in the variable 'tuned_dt'. 
print(tuned_rf)
print(tuned_lr)

"""# 10.0 Plot a Model

Before model finalization, the `plot_model()` function can be used to analyze the performance across different aspects such as AUC, confusion_matrix, decision boundary etc. This function takes a trained model object and returns a plot based on the test / hold-out set. 

There are 15 different plots available, please see the `plot_model()` docstring for the list of available plots.

### 10.1 AUC Plot
"""

plot_model(tuned_lr)

#?plot_model
plot_model(tuned_lr, plot = 'feature')

plot_model(tuned_rf, plot = 'auc')

"""### 10.2 Precision-Recall Curve"""

plot_model(tuned_dt, plot = 'pr')

"""### 10.3 Feature Importance Plot

### 10.4 Confusion Matrix
"""

plot_model(tuned_dt, plot = 'confusion_matrix')

import sklearn

?tuned_dt

sklearn.tree.plot_tree(tuned_dt)

plot_model(tuned_lr,plot="confusion_matrix")



"""*Another* way to analyze the performance of models is to use the `evaluate_model()` function which displays a user interface for all of the available plots for a given model. It internally uses the `plot_model()` function. """

evaluate_model(tuned_lr)

"""# 11.0 Predict on test / hold-out Sample

Before finalizing the model, it is advisable to perform one final check by predicting the test/hold-out set and reviewing the evaluation metrics. If you look at the information grid in Section 6 above, you will see that 30% (6,841 samples) of the data has been separated out as test/hold-out sample. All of the evaluation metrics we have seen above are cross validated results based on the training set (70%) only. Now, using our final trained model stored in the `tuned_lr` variable we will predict against the hold-out sample and evaluate the metrics to see if they are materially different than the CV results.
"""

predict_model(tuned_lr);

predict_model(tuned_rf);

"""Model finalization is the last step in the experiment. A normal machine learning workflow in PyCaret starts with `setup()`, followed by comparing all models using `compare_models()` and shortlisting a few candidate models (based on the metric of interest) to perform several modeling techniques such as hyperparameter tuning, ensembling, stacking etc. This workflow will eventually lead you to the best model for use in making predictions on new and unseen data. The `finalize_model()` function fits the model onto the complete dataset including the test/hold-out sample (20% in this case). The purpose of this function is to train the model on the complete dataset before it is deployed in production.

# 12.0 Finalize Model for Deployment
"""

final_lr = finalize_model(tuned_lr)
final_rf =finalize_model(tuned_rf)

#Final Random Forest model parameters for deployment
print(final_lr)

predict_model(final_lr)
predict_model(final_rf)

"""Notice how the AUC in `final_rf` has increased to **`0.8189`** from **`0.7538`**, even though the model is the same. This is because the `final_rf` variable has been trained on the complete dataset including the test/hold-out set.

# 13.0 Predict on unseen data

The `predict_model()` function is also used to predict on the unseen dataset. The only difference from section 11 above is that this time we will pass the `data_unseen` parameter. `data_unseen` is the variable created at the beginning of the tutorial and contains 5% (1200 samples) of the original dataset which was never exposed to PyCaret. (see section 5 for explanation)
"""

unseen_predictions = predict_model(final_lr, data=data_unseen)
unseen_predictions.head()

"""The `Label` and `Score` columns are added onto the `data_unseen` set. Label is the prediction and score is the probability of the prediction. Notice that predicted results are concatenated to the original dataset while all the transformations are automatically performed in the background.

# 14.0 Saving the model

We have now finished the experiment by finalizing the `tuned_rf` model which is now stored in `final_rf` variable. We have also used the model stored in `final_rf` to predict `data_unseen`. This brings us to the end of our experiment, but one question is still to be asked: What happens when you have more new data to predict? Do you have to go through the entire experiment again? The answer is no, PyCaret's inbuilt function `save_model()` allows you to save the model along with entire transformation pipeline for later use.
"""

save_model(final_lr,'Final logistic classification-heroku_version')
save_model(final_rf,'Final random forest-heroku_version')

"""(TIP : It's always good to use date in the filename when saving models, it's good for version control.)

# 15.0 Loading the saved model

To load a saved model at a future date in the same or an alternative environment, we would use PyCaret's `load_model()` function and then easily apply the saved model on new unseen data for prediction.
"""

saved_final_lr = load_model('Final logistic classification- 1 cleaned data')

"""Once the model is loaded in the environment, you can simply use it to predict on any new data using the same `predict_model()` function. Below we have applied the loaded model to predict the same `data_unseen` that we used in section 13 above."""

new_prediction = predict_model(saved_final_lr, data=data_unseen)
new_prediction.head

dfp=pd.DataFrame(new_prediction)
dfp.to_csv('prediction-Logistic Regression-1st clean data.csv')

"""Notice that the results of `unseen_predictions` and `new_prediction` are identical.

This tutorial has covered the entire machine learning pipeline from data ingestion, pre-processing, training the model, hyperparameter tuning, prediction and saving the model for later use. We have completed all of these steps in less than 10 commands which are naturally constructed and very intuitive to remember such as `create_model()`, `tune_model()`, `compare_models()`. Re-creating the entire experiment without PyCaret would have taken well over 100 lines of code in most libraries.

We have only covered the basics of `pycaret.classification`. In following tutorials we will go deeper into advanced pre-processing, ensembling, generalized stacking and other techniques that allow you to fully customize your machine learning pipeline and are must know for any data scientist.

See you at the next tutorial. Follow the link to __[Binary Classification Tutorial (CLF102) - Intermediate Level](https://github.com/pycaret/pycaret/blob/master/Tutorials/Binary%20Classification%20Tutorial%20Level%20Intermediate%20-%20CLF102.ipynb)__

This is a preliminary run on model comparison
We can do the same in R and compare accuracy
"""